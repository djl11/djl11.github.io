<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>

    <meta name=viewport content=“width=800”>
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
        /* latin-ext */
        @font-face {
            font-family: 'Lato';
            font-style: italic;
            font-weight: 400;
            src: local('Lato Italic'), local('Lato-Italic'), url(https://fonts.gstatic.com/s/lato/v15/S6u8w4BMUTPHjxsAUi-qNiXg7eU0.woff2) format('woff2');
            unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
        }

        /* latin */
        @font-face {
            font-family: 'Lato';
            font-style: italic;
            font-weight: 400;
            src: local('Lato Italic'), local('Lato-Italic'), url(https://fonts.gstatic.com/s/lato/v15/S6u8w4BMUTPHjxsAXC-qNiXg7Q.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
        }

        /* latin-ext */
        @font-face {
            font-family: 'Lato';
            font-style: italic;
            font-weight: 700;
            src: local('Lato Bold Italic'), local('Lato-BoldItalic'), url(https://fonts.gstatic.com/s/lato/v15/S6u_w4BMUTPHjxsI5wq_FQftx9897sxZ.woff2) format('woff2');
            unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
        }

        /* latin */
        @font-face {
            font-family: 'Lato';
            font-style: italic;
            font-weight: 700;
            src: local('Lato Bold Italic'), local('Lato-BoldItalic'), url(https://fonts.gstatic.com/s/lato/v15/S6u_w4BMUTPHjxsI5wq_Gwftx9897g.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
        }

        /* latin-ext */
        @font-face {
            font-family: 'Lato';
            font-style: normal;
            font-weight: 400;
            src: local('Lato Regular'), local('Lato-Regular'), url(https://fonts.gstatic.com/s/lato/v15/S6uyw4BMUTPHjxAwXiWtFCfQ7A.woff2) format('woff2');
            unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
        }

        /* latin */
        @font-face {
            font-family: 'Lato';
            font-style: normal;
            font-weight: 400;
            src: local('Lato Regular'), local('Lato-Regular'), url(https://fonts.gstatic.com/s/lato/v15/S6uyw4BMUTPHjx4wXiWtFCc.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
        }

        /* latin-ext */
        @font-face {
            font-family: 'Lato';
            font-style: normal;
            font-weight: 700;
            src: local('Lato Bold'), local('Lato-Bold'), url(https://fonts.gstatic.com/s/lato/v15/S6u9w4BMUTPHh6UVSwaPGQ3q5d0N7w.woff2) format('woff2');
            unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
        }

        /* latin */
        @font-face {
            font-family: 'Lato';
            font-style: normal;
            font-weight: 700;
            src: local('Lato Bold'), local('Lato-Bold'), url(https://fonts.gstatic.com/s/lato/v15/S6u9w4BMUTPHh6UVSwiPGQ3q5d0.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
        }

        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus,
        a:hover {
            color: #f09228;
            text-decoration: none;
        }

        body,
        td,
        th,
        tr,
        p,
        a {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px
        }

        strong {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px;
        }

        heading {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 22px;
        }

        papertitle {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px;
            font-weight: 700
        }

        name {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 32px;
        }

        .one {
            width: 160px;
            height: 160px;
            position: relative;
        }

        .two {
            width: 160px;
            height: 160px;
            position: absolute;
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        .fade {
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        span.highlight {
            background-color: #ffffd0;
        }
    </style>
    <link rel="icon" type="image/jpg" href="img/headshot.jpg">
    <title>Daniel Lenton</title>
    <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
    <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet'
          type='text/css'>

</head>
<body>
<table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
        <td>
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td width="67%" valign="middle">
                        <p align="center">
                            <name>Daniel Lenton</name>
                        </p>
                        <p>
                            Creator of <a href="https://github.com/unifyai/ivy">Ivy</a>, where we're on a mission to unify the fragmented AI frameworks, infrastructure and hardware. Learn more at <a href="https://unify.ai/">unify.ai</a>
                        </p>
                        <p align=center>
                            <a href="https://lets-unify.ai">Ivy</a>
                            &nbsp/&nbsp
                            <a href="https://medium.com/@unifyai">Medium</a>
                            &nbsp/&nbsp
                            <a href="https://www.linkedin.com/in/daniellenton/">LinkedIn</a>
                            &nbsp/&nbsp
                            <a href="https://twitter.com/DanielLenton1">Twitter</a>
                            &nbsp/&nbsp
                            <a href="https://scholar.google.com/citations?user=KwpECyQAAAAJ">Google Scholar</a>
                            &nbsp/&nbsp
                            <a href="mailto:djl11@ic.ac.uk">Email</a>
                        </p>
                    </td>
                    <td width="33%">
                        <a href="img/headshot.jpg">
                            <img src="img/headshot.jpg" width="250px"></a>
                    </td>
                </tr>
            </table>
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td width="100%" valign="middle">
                        <heading>Research and Publications</heading>
                    </td>
                </tr>
            </table>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

                <tr>
                    <td width="25%">
                        <div class="one">
                            <div class="two"><img src='img/esm.png' width="160"></div>
                        </div>
                    </td>
                    <td valign="top" width="75%">
                        <a href="https://djl11.github.io/ESM/">
                            <papertitle>End-to-End Egospheric Spatial Memory</papertitle>
                        </a>
                        <br>
                        <strong>Daniel lenton</strong>,
                        <a href="https://stepjam.github.io/">Stephen James</a>,
                        <a href="http://www.ronnieclark.co.uk/">Ronald Clark</a>,
                        <a href="https://www.doc.ic.ac.uk/~ajd/">Andrew Davison</a>
                        <br>
                        <em>International Conference on Learning Representations (<b>ICLR</b>)</em>, 2021
                        <br>
                        <a href="https://arxiv.org/abs/2102.07764">paper</a> /
                        <a href="https://youtu.be/TvprqJHngTM">video</a> /
                        <a href="https://github.com/unifyai/memory">code</a> /
                        <a href="https://djl11.github.io/ESM/">project page</a>
                        <p></p>
                        <p>ESM encodes the memory in an ego-sphere around the agent, enabling expressive 3D representations.
                           ESM can be trained end-to-end via either imitation or reinforcement learning,
                           and improves both training efficiency and final performance against other memory baselines on visuomotor control tasks.</p>
                    </td>
                </tr>

                <tr>
                    <td width="25%">
                        <div class="one">
                            <div class="two"><img src='img/ivy.png' width="160"></div>
                        </div>
                    </td>
                    <td valign="top" width="75%">
                        <a href="https://lets-unify.ai/">
                            <papertitle>Ivy: Templated Deep Learning for Inter-Framework Portability</papertitle>
                        </a>
                        <br>
                        <strong>Daniel lenton</strong>,
                        <a href="https://fabiopardo.github.io/">Fabio Pardo</a>,
                        <a href="http://fabianfalck.com/">Fabian Falck</a>,
                        <a href="https://stepjam.github.io/">Stephen James</a>,
                        <a href="http://www.ronnieclark.co.uk/">Ronald Clark</a>
                        <br>
                        <em>arXiv</em>, 2021
                        <br>
                        <a href="https://arxiv.org/abs/2102.02886">paper</a> /
                        <a href="https://www.youtube.com/watch?v=AympTZtQAE8&t=2s">video</a> /
                        <a href="https://github.com/unifyai/ivy">code</a> /
                        <a href="https://lets-unify.ai/">project page</a>
                        <p></p>
                        <p>Ivy is a templated Machine Learning (ML) framework which abstracts existing ML frameworks such that their core functions all exhibit consistent call signatures and
                            syntax. Ivy allows high-level framework-agnostic functions, layers and libraries to be implemented on top.</p>
                    </td>
                </tr>

                <tr>
                    <td width="25%">
                        <div class="one">
                            <div class="two"><img src='img/shortest_paths.png' width="160"></div>
                        </div>
                    </td>
                    <td valign="top" width="75%">
                        <a href="https://mpmisko.github.io/unsupervised_prn_page">
                            <papertitle>Unsupervised Path Regression Networks</papertitle>
                        </a>
                        <br>
                        Michal Pandy,
                        <strong>Daniel lenton</strong>,
                        <a href="http://www.ronnieclark.co.uk/">Ronald Clark</a>
                        <br>
                        <em>International Conference on Intelligent Robots and Systems (<b>IROS</b>)</em>, 2021
                        <br>
                        <a href="https://arxiv.org/abs/2011.14787">paper</a> /
                        <a href="https://mpmisko.github.io/unsupervised_prn_page">project page</a>
                        <p></p>
                        <p>We demonstrate that challenging shortest path problems can be solved via direct spline
                            regression from a neural network, trained in an unsupervised manner without requiring
                            ground truth optimal paths for training. To achieve this, we derive a geometry-dependent
                            optimal cost function whose minima guarantees collision-free solutions.</p>
                    </td>
                </tr>

                <tr>
                    <td width="25%">
                        <div class="one">
                            <div class="two"><img src='img/WPN.png' width="160"></div>
                        </div>
                    </td>
                    <td valign="top" width="75%">
                        <a href="https://sites.google.com/view/waypoint-planning-networks">
                            <papertitle>Waypoint Planning Networks</papertitle>
                        </a>
                        <br>
                        Alexandru-Iosif Toma,
                        Hussein Ali Jaafar,
                        Hao-Ya Hsueh,
                        <a href="https://stepjam.github.io/">Stephen James</a>,
                        <strong>Daniel lenton</strong>,
                        <a href="http://www.ronnieclark.co.uk/">Ronald Clark</a>,
                        <a href="https://www.ryerson.ca/mechanical-industrial/people/faculty/sajad-saeedi/">Sajad Saeedi</a>,
                        <br>
                        <em>International Conference on Computer Vision and Robotics (<b>CVR</b>)</em>, 2021
                        <br>
                        <a href="https://arxiv.org/abs/2105.00312">paper</a> /
                        <a href="https://www.youtube.com/watch?v=e_enK3SkHU4&ab_channel=Robotics%26ComputerVisionLab">video</a> /
                        <a href="https://github.com/husseinalijaafar/WPN">code</a> /
                        <a href="https://sites.google.com/view/waypoint-planning-networks">project page</a> /
                        <p></p>
                        <p>Waypoint Planning Networks, or WPN, is a hybrid motion planning algorithm based on LSTMs with
                            a local kernel, a classic algorithm such as A*, and a global kernel using a learned
                            algorithm.  WPN produces a more computationally efficient and robust solution than other
                            learned approaches.</p>
                    </td>
                </tr>

                <tr>
                    <td width="25%">
                        <div class="one">
                            <div class="two"><img src='img/more_fusion.jpg' width="160"></div>
                        </div>
                    </td>
                    <td valign="top" width="75%">
                        <a href="https://morefusion.wkentaro.com/">
                            <papertitle>MoreFusion: Multi-object Reasoning for 6D Pose Estimation from Volumetric
                                Fusion
                            </papertitle>
                        </a>
                        <br>
                        <a href="https://wkentaro.com/">Kentaro Wada</a>,
                        Edgar Sucar,
                        <a href="https://stepjam.github.io/">Stephen James</a>,
                        <strong>Daniel lenton</strong>,
                        <a href="https://www.doc.ic.ac.uk/~ajd/">Andrew Davison</a>
                        <br>
                        <em>Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2020
                        <br>
                        <a href="https://arxiv.org/abs/2004.04336">paper</a> /
                        <a href="https://www.youtube.com/watch?v=6oLUhuZL4ko">video</a> /
                        <a href="https://github.com/wkentaro/morefusion">code</a> /
                        <a href="https://morefusion.wkentaro.com/">project page</a>
                        <p></p>
                        <p>MoreFusion makes 3D object pose proposals from single RGB-D views,
                            accumulates pose estimates and non-parametric occupancy information from multiple views as
                            the camera moves,
                            and performs joint optimization to estimate consistent, non-intersecting poses for multiple
                            objects in contact.</p>
                    </td>
                </tr>

            </table>

        </td>
    </tr>
</table>
</body>
</html>
