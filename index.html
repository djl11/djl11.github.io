<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>

    <meta name=viewport content=“width=800”>
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
        /* latin-ext */
        @font-face {
            font-family: 'Lato';
            font-style: italic;
            font-weight: 400;
            src: local('Lato Italic'), local('Lato-Italic'), url(https://fonts.gstatic.com/s/lato/v15/S6u8w4BMUTPHjxsAUi-qNiXg7eU0.woff2) format('woff2');
            unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
        }

        /* latin */
        @font-face {
            font-family: 'Lato';
            font-style: italic;
            font-weight: 400;
            src: local('Lato Italic'), local('Lato-Italic'), url(https://fonts.gstatic.com/s/lato/v15/S6u8w4BMUTPHjxsAXC-qNiXg7Q.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
        }

        /* latin-ext */
        @font-face {
            font-family: 'Lato';
            font-style: italic;
            font-weight: 700;
            src: local('Lato Bold Italic'), local('Lato-BoldItalic'), url(https://fonts.gstatic.com/s/lato/v15/S6u_w4BMUTPHjxsI5wq_FQftx9897sxZ.woff2) format('woff2');
            unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
        }

        /* latin */
        @font-face {
            font-family: 'Lato';
            font-style: italic;
            font-weight: 700;
            src: local('Lato Bold Italic'), local('Lato-BoldItalic'), url(https://fonts.gstatic.com/s/lato/v15/S6u_w4BMUTPHjxsI5wq_Gwftx9897g.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
        }

        /* latin-ext */
        @font-face {
            font-family: 'Lato';
            font-style: normal;
            font-weight: 400;
            src: local('Lato Regular'), local('Lato-Regular'), url(https://fonts.gstatic.com/s/lato/v15/S6uyw4BMUTPHjxAwXiWtFCfQ7A.woff2) format('woff2');
            unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
        }

        /* latin */
        @font-face {
            font-family: 'Lato';
            font-style: normal;
            font-weight: 400;
            src: local('Lato Regular'), local('Lato-Regular'), url(https://fonts.gstatic.com/s/lato/v15/S6uyw4BMUTPHjx4wXiWtFCc.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
        }

        /* latin-ext */
        @font-face {
            font-family: 'Lato';
            font-style: normal;
            font-weight: 700;
            src: local('Lato Bold'), local('Lato-Bold'), url(https://fonts.gstatic.com/s/lato/v15/S6u9w4BMUTPHh6UVSwaPGQ3q5d0N7w.woff2) format('woff2');
            unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
        }

        /* latin */
        @font-face {
            font-family: 'Lato';
            font-style: normal;
            font-weight: 700;
            src: local('Lato Bold'), local('Lato-Bold'), url(https://fonts.gstatic.com/s/lato/v15/S6u9w4BMUTPHh6UVSwiPGQ3q5d0.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
        }

        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus,
        a:hover {
            color: #f09228;
            text-decoration: none;
        }

        body,
        td,
        th,
        tr,
        p,
        a {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px
        }

        strong {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px;
        }

        heading {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 22px;
        }

        papertitle {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px;
            font-weight: 700
        }

        name {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 32px;
        }

        .one {
            width: 160px;
            height: 160px;
            position: relative;
        }

        .two {
            width: 160px;
            height: 160px;
            position: absolute;
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        .fade {
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        span.highlight {
            background-color: #ffffd0;
        }
    </style>
    <link rel="icon" type="image/jpg" href="img/headshot.jpg">
    <title>Daniel Lenton</title>
    <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
    <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet'
          type='text/css'>

</head>
<body>
<table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
        <td>
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td width="67%" valign="middle">
                        <p align="center">
                            <name>Daniel Lenton</name>
                        </p>
                        <p>
                            I'm a PhD student in <a href="https://www.imperial.ac.uk/dyson-robotics-lab">The Dyson
                            Robotics Lab</a> at <a href="https://www.imperial.ac.uk/">Imperial College London</a>.
                            In my research, I'm exploring the intersection between learning-based geometric reresentations,
                            ego-centric perception, spatial memory, and reinforcement learning for robotics.
                        </p>
                        <p>
                            Task-driven optimization is useful for streamlining learnt geometric representations to be directly useful for acting in the world,
                            the principles of 3D reconstruction can increase structure in the computation graph, improving generalization over more naive networks, and
                            egocentric representations are useful for learning to select actions in the world.
                        </p>
                        <p>
                            In addition to my research, I am also a teaching assistant at Imperial College London.
                            I have taught courses on robot navigation, algorithms, intro to AI, deep learning and software engineering design.
                        </p>
                        <p align=center>
                            <a href="mailto:djl11@ic.ac.uk">Email</a> &nbsp/&nbsp
                            <!--<a href="pdf/cv.pdf">CV</a> &nbsp/&nbsp-->
                            <a href="https://scholar.google.com/citations?user=KwpECyQAAAAJ">Google Scholar</a>
                            &nbsp/&nbsp
                            <a href="https://twitter.com/DanielLenton1">Twitter</a>
                        </p>
                    </td>
                    <td width="33%">
                        <a href="img/headshot.jpg">
                            <img src="img/headshot.jpg" width="250px"></a>
                    </td>
                </tr>
            </table>
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td width="100%" valign="middle">
                        <heading>Research and Publications</heading>
                    </td>
                </tr>
            </table>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

                <tr>
                    <td width="25%">
                        <div class="one">
                            <div class="two"><img src='img/esm.png' width="160"></div>
                        </div>
                    </td>
                    <td valign="top" width="75%">
                        <a href="https://djl11.github.io/ESM/">
                            <papertitle>End-to-End Egospheric Spatial Memory</papertitle>
                        </a>
                        <br>
                        <strong>Daniel lenton</strong>,
                        <a href="https://stepjam.github.io/">Stephen James</a>,
                        <a href="http://www.ronnieclark.co.uk/">Ronald Clark</a>,
                        <a href="https://www.doc.ic.ac.uk/~ajd/">Andrew Davison</a>
                        <br>
                        <em>International Conference on Learning Representations (<b>ICLR</b>)</em>, 2021
                        <br>
                        <a href="https://arxiv.org/abs/2102.07764">paper</a> /
                        <a href="https://youtu.be/TvprqJHngTM">video</a> /
                        <a href="https://github.com/ivy-dl/memory">code</a> /
                        <a href="https://djl11.github.io/ESM/">project page</a>
                        <p></p>
                        <p>ESM encodes the memory in an ego-sphere around the agent, enabling expressive 3D representations.
                           ESM can be trained end-to-end via either imitation or reinforcement learning,
                           and improves both training efficiency and final performance against other memory baselines on visuomotor control tasks.</p>
                    </td>
                </tr>

                <tr>
                    <td width="25%">
                        <div class="one">
                            <div class="two"><img src='img/ivy.png' width="160"></div>
                        </div>
                    </td>
                    <td valign="top" width="75%">
                        <a href="https://ivy-dl.org/">
                            <papertitle>Ivy: Templated Deep Learning for Inter-Framework Portability</papertitle>
                        </a>
                        <br>
                        <strong>Daniel lenton</strong>,
                        <a href="https://fabiopardo.github.io/">Fabio Pardo</a>,
                        <a href="http://fabianfalck.com/">Fabian Falck</a>,
                        <a href="https://stepjam.github.io/">Stephen James</a>,
                        <a href="http://www.ronnieclark.co.uk/">Ronald Clark</a>
                        <br>
                        <em>arXiv</em>, 2021
                        <br>
                        <a href="https://arxiv.org/abs/2102.02886">paper</a> /
                        <a href="https://www.youtube.com/watch?v=AympTZtQAE8&t=2s">video</a> /
                        <a href="https://github.com/ivy-dl/ivy">code</a> /
                        <a href="https://ivy-dl.org/">project page</a>
                        <p></p>
                        <p>Ivy is a templated Deep Learning (DL) framework which abstracts existing DL frameworks such that their core functions all exhibit consistent call signatures,
                            syntax and input-output behaviour. Ivy allows high-level framework-agnostic functions to be implemented through the use of framework templates.</p>
                    </td>
                </tr>

                <tr>
                    <td width="25%">
                        <div class="one">
                            <div class="two"><img src='img/shortest_paths.png' width="160"></div>
                        </div>
                    </td>
                    <td valign="top" width="75%">
                        <a href="https://arxiv.org/abs/2011.14787">
                            <papertitle>Learning To Find Shortest Collision-Free Paths From Images</papertitle>
                        </a>
                        <br>
                        Michal Pándy,
                        <strong>Daniel lenton</strong>,
                        <a href="http://www.ronnieclark.co.uk/">Ronald Clark</a>
                        <br>
                        <em>arXiv</em>, 2020
                        <br>
                        <a href="https://arxiv.org/abs/2011.14787">paper</a>
                        <p></p>
                        <p>We propose a novel cost function that guarantees collision-free shortest paths are found at
                            its minimum.
                            We show that our approach works seamlessly with RGBD input and predicts high-quality paths
                            in 2D, 3D, and 6 DoF robotic manipulator settings.</p>
                    </td>
                </tr>

                <tr>
                    <td width="25%">
                        <div class="one">
                            <div class="two"><img src='img/more_fusion.jpg' width="160"></div>
                        </div>
                    </td>
                    <td valign="top" width="75%">
                        <a href="https://morefusion.wkentaro.com/">
                            <papertitle>MoreFusion: Multi-object Reasoning for 6D Pose Estimation from Volumetric
                                Fusion
                            </papertitle>
                        </a>
                        <br>
                        <a href="https://wkentaro.com/">Kentaro Wada</a>,
                        Edgar Sucar,
                        <a href="https://stepjam.github.io/">Stephen James</a>,
                        <strong>Daniel lenton</strong>,
                        <a href="https://www.doc.ic.ac.uk/~ajd/">Andrew Davison</a>
                        <br>
                        <em>Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2020
                        <br>
                        <a href="https://arxiv.org/abs/2004.04336">paper</a> /
                        <a href="https://www.youtube.com/watch?v=6oLUhuZL4ko">video</a> /
                        <a href="https://github.com/wkentaro/morefusion">code</a> /
                        <a href="https://morefusion.wkentaro.com/">project page</a>
                        <p></p>
                        <p>MoreFusion makes 3D object pose proposals from single RGB-D views,
                            accumulates pose estimates and non-parametric occupancy information from multiple views as
                            the camera moves,
                            and performs joint optimization to estimate consistent, non-intersecting poses for multiple
                            objects in contact.</p>
                    </td>
                </tr>

            </table>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td>
                        <br>
                        <p align="right">
                            <font size="2">
                                Website template by <a href="https://jonbarron.info">Jon Barron</a>.
                            </font>
                        </p>
                    </td>
                </tr>
            </table>

        </td>
    </tr>
</table>
</body>
</html>
